# -*- coding: utf-8 -*-
"""
===================================
Aè‚¡è‡ªé€‰è‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿ - æ ¸å¿ƒåˆ†ææµæ°´çº¿
===================================

èŒè´£ï¼š
1. ç®¡ç†æ•´ä¸ªåˆ†ææµç¨‹
2. åè°ƒæ•°æ®è·å–ã€å­˜å‚¨ã€æœç´¢ã€åˆ†æã€é€šçŸ¥ç­‰æ¨¡å—
3. å®ç°å¹¶å‘æ§åˆ¶å’Œå¼‚å¸¸å¤„ç†
4. æä¾›è‚¡ç¥¨åˆ†æçš„æ ¸å¿ƒåŠŸèƒ½
"""

import logging
import time
import uuid
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import date, timedelta
from typing import List, Dict, Any, Optional, Tuple

import pandas as pd

from src.config import get_config, Config
from src.storage import get_db
from data_provider import DataFetcherManager
from data_provider.realtime_types import ChipDistribution
from src.analyzer import GeminiAnalyzer, AnalysisResult, STOCK_NAME_MAP
from src.notification import NotificationService, NotificationChannel
from src.search_service import SearchService
from src.enums import ReportType
from src.stock_analyzer import StockTrendAnalyzer, TrendAnalysisResult
from src.core.trading_calendar import get_market_for_stock, is_market_open
from bot.models import BotMessage


logger = logging.getLogger(__name__)


class StockAnalysisPipeline:
    """
    è‚¡ç¥¨åˆ†æä¸»æµç¨‹è°ƒåº¦å™¨
    
    èŒè´£ï¼š
    1. ç®¡ç†æ•´ä¸ªåˆ†ææµç¨‹
    2. åè°ƒæ•°æ®è·å–ã€å­˜å‚¨ã€æœç´¢ã€åˆ†æã€é€šçŸ¥ç­‰æ¨¡å—
    3. å®ç°å¹¶å‘æ§åˆ¶å’Œå¼‚å¸¸å¤„ç†
    """
    
    def __init__(
        self,
        config: Optional[Config] = None,
        max_workers: Optional[int] = None,
        source_message: Optional[BotMessage] = None,
        query_id: Optional[str] = None,
        query_source: Optional[str] = None,
        save_context_snapshot: Optional[bool] = None
    ):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        """
        self.config = config or get_config()
        self.max_workers = max_workers or self.config.max_workers
        self.source_message = source_message
        self.query_id = query_id
        self.query_source = self._resolve_query_source(query_source)
        self.save_context_snapshot = (
            self.config.save_context_snapshot if save_context_snapshot is None else save_context_snapshot
        )
        
        # åˆå§‹åŒ–å„æ¨¡å—
        self.db = get_db()
        self.fetcher_manager = DataFetcherManager()
        # ä¸å†å•ç‹¬åˆ›å»º akshare_fetcherï¼Œç»Ÿä¸€ä½¿ç”¨ fetcher_manager è·å–å¢å¼ºæ•°æ®
        self.trend_analyzer = StockTrendAnalyzer()  # è¶‹åŠ¿åˆ†æå™¨
        self.analyzer = GeminiAnalyzer()
        self.notifier = NotificationService(source_message=source_message)
        
        # åˆå§‹åŒ–æœç´¢æœåŠ¡
        self.search_service = SearchService(
            bocha_keys=self.config.bocha_api_keys,
            tavily_keys=self.config.tavily_api_keys,
            brave_keys=self.config.brave_api_keys,
            serpapi_keys=self.config.serpapi_keys,
            news_max_age_days=self.config.news_max_age_days,
        )
        
        logger.info(f"è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å¹¶å‘æ•°: {self.max_workers}")
        logger.info("å·²å¯ç”¨è¶‹åŠ¿åˆ†æå™¨ (MA5>MA10>MA20 å¤šå¤´åˆ¤æ–­)")
        # æ‰“å°å®æ—¶è¡Œæƒ…/ç­¹ç é…ç½®çŠ¶æ€
        if self.config.enable_realtime_quote:
            logger.info(f"å®æ—¶è¡Œæƒ…å·²å¯ç”¨ (ä¼˜å…ˆçº§: {self.config.realtime_source_priority})")
        else:
            logger.info("å®æ—¶è¡Œæƒ…å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨å†å²æ”¶ç›˜ä»·")
        if self.config.enable_chip_distribution:
            logger.info("ç­¹ç åˆ†å¸ƒåˆ†æå·²å¯ç”¨")
        else:
            logger.info("ç­¹ç åˆ†å¸ƒåˆ†æå·²ç¦ç”¨")
        if self.search_service.is_available:
            logger.info("æœç´¢æœåŠ¡å·²å¯ç”¨ (Tavily/SerpAPI)")
        else:
            logger.warning("æœç´¢æœåŠ¡æœªå¯ç”¨ï¼ˆæœªé…ç½® API Keyï¼‰")
    
    def fetch_and_save_stock_data(
        self, 
        code: str,
        force_refresh: bool = False
    ) -> Tuple[bool, Optional[str]]:
        """
        è·å–å¹¶ä¿å­˜å•åªè‚¡ç¥¨æ•°æ®
        
        æ–­ç‚¹ç»­ä¼ é€»è¾‘ï¼š
        1. æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²æœ‰ä»Šæ—¥æ•°æ®
        2. å¦‚æœæœ‰ä¸”ä¸å¼ºåˆ¶åˆ·æ–°ï¼Œåˆ™è·³è¿‡ç½‘ç»œè¯·æ±‚
        3. å¦åˆ™ä»æ•°æ®æºè·å–å¹¶ä¿å­˜
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥æœ¬åœ°ç¼“å­˜ï¼‰
            
        Returns:
            Tuple[æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯]
        """
        try:
            # é¦–å…ˆè·å–è‚¡ç¥¨åç§°
            stock_name = self.fetcher_manager.get_stock_name(code)

            today = date.today()
            # æ³¨æ„ï¼šè¿™é‡Œç”¨è‡ªç„¶æ—¥ date.today() åšâ€œæ–­ç‚¹ç»­ä¼ â€åˆ¤æ–­ã€‚
            # è‹¥åœ¨å‘¨æœ«/èŠ‚å‡æ—¥/éäº¤æ˜“æ—¥è¿è¡Œï¼Œæˆ–æœºå™¨æ—¶åŒºä¸åœ¨ä¸­å›½ï¼Œå¯èƒ½å‡ºç°ï¼š
            # - æ•°æ®åº“å·²æœ‰æœ€æ–°äº¤æ˜“æ—¥æ•°æ®ä½†ä»ä¼šé‡å¤æ‹‰å–ï¼ˆhas_today_data è¿”å› Falseï¼‰
            # - æˆ–åœ¨è·¨æ—¥/æ—¶åŒºåç§»æ—¶è¯¯åˆ¤â€œä»Šæ—¥å·²æœ‰æ•°æ®â€
            # è¯¥è¡Œä¸ºç›®å‰ä¿ç•™ï¼ˆæŒ‰éœ€æ±‚ä¸æ”¹é€»è¾‘ï¼‰ï¼Œä½†å¦‚éœ€æ›´ä¸¥è°¨å¯æ”¹ä¸ºâ€œæœ€æ–°äº¤æ˜“æ—¥/æ•°æ®æºæœ€æ–°æ—¥æœŸâ€åˆ¤æ–­ã€‚
            
            # æ–­ç‚¹ç»­ä¼ æ£€æŸ¥ï¼šå¦‚æœä»Šæ—¥æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡
            if not force_refresh and self.db.has_today_data(code, today):
                logger.info(f"{stock_name}({code}) ä»Šæ—¥æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡è·å–ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰")
                return True, None

            # ä»æ•°æ®æºè·å–æ•°æ®
            logger.info(f"{stock_name}({code}) å¼€å§‹ä»æ•°æ®æºè·å–æ•°æ®...")
            df, source_name = self.fetcher_manager.get_daily_data(code, days=30)

            if df is None or df.empty:
                return False, "è·å–æ•°æ®ä¸ºç©º"

            # ä¿å­˜åˆ°æ•°æ®åº“
            saved_count = self.db.save_daily_data(df, code, source_name)
            logger.info(f"{stock_name}({code}) æ•°æ®ä¿å­˜æˆåŠŸï¼ˆæ¥æº: {source_name}ï¼Œæ–°å¢ {saved_count} æ¡ï¼‰")

            return True, None

        except Exception as e:
            error_msg = f"è·å–/ä¿å­˜æ•°æ®å¤±è´¥: {str(e)}"
            logger.error(f"{stock_name}({code}) {error_msg}")
            return False, error_msg
    
    def analyze_stock(self, code: str, report_type: ReportType, query_id: str) -> Optional[AnalysisResult]:
        """
        åˆ†æå•åªè‚¡ç¥¨ï¼ˆå¢å¼ºç‰ˆï¼šå«é‡æ¯”ã€æ¢æ‰‹ç‡ã€ç­¹ç åˆ†æã€å¤šç»´åº¦æƒ…æŠ¥ï¼‰
        
        æµç¨‹ï¼š
        1. è·å–å®æ—¶è¡Œæƒ…ï¼ˆé‡æ¯”ã€æ¢æ‰‹ç‡ï¼‰- é€šè¿‡ DataFetcherManager è‡ªåŠ¨æ•…éšœåˆ‡æ¢
        2. è·å–ç­¹ç åˆ†å¸ƒ - é€šè¿‡ DataFetcherManager å¸¦ç†”æ–­ä¿æŠ¤
        3. è¿›è¡Œè¶‹åŠ¿åˆ†æï¼ˆåŸºäºäº¤æ˜“ç†å¿µï¼‰
        4. å¤šç»´åº¦æƒ…æŠ¥æœç´¢ï¼ˆæœ€æ–°æ¶ˆæ¯+é£é™©æ’æŸ¥+ä¸šç»©é¢„æœŸï¼‰
        5. ä»æ•°æ®åº“è·å–åˆ†æä¸Šä¸‹æ–‡
        6. è°ƒç”¨ AI è¿›è¡Œç»¼åˆåˆ†æ
        
        Args:
            query_id: æŸ¥è¯¢é“¾è·¯å…³è” id
            code: è‚¡ç¥¨ä»£ç 
            report_type: æŠ¥å‘Šç±»å‹
            
        Returns:
            AnalysisResult æˆ– Noneï¼ˆå¦‚æœåˆ†æå¤±è´¥ï¼‰
        """
        try:
            # è·å–è‚¡ç¥¨åç§°ï¼ˆä¼˜å…ˆä»å®æ—¶è¡Œæƒ…è·å–çœŸå®åç§°ï¼‰
            stock_name = self.fetcher_manager.get_stock_name(code)

            # Step 1: è·å–å®æ—¶è¡Œæƒ…ï¼ˆé‡æ¯”ã€æ¢æ‰‹ç‡ç­‰ï¼‰- ä½¿ç”¨ç»Ÿä¸€å…¥å£ï¼Œè‡ªåŠ¨æ•…éšœåˆ‡æ¢
            realtime_quote = None
            try:
                realtime_quote = self.fetcher_manager.get_realtime_quote(code)
                if realtime_quote:
                    # ä½¿ç”¨å®æ—¶è¡Œæƒ…è¿”å›çš„çœŸå®è‚¡ç¥¨åç§°
                    if realtime_quote.name:
                        stock_name = realtime_quote.name
                    # å…¼å®¹ä¸åŒæ•°æ®æºçš„å­—æ®µï¼ˆæœ‰äº›æ•°æ®æºå¯èƒ½æ²¡æœ‰ volume_ratioï¼‰
                    volume_ratio = getattr(realtime_quote, 'volume_ratio', None)
                    turnover_rate = getattr(realtime_quote, 'turnover_rate', None)
                    logger.info(f"{stock_name}({code}) å®æ—¶è¡Œæƒ…: ä»·æ ¼={realtime_quote.price}, "
                              f"é‡æ¯”={volume_ratio}, æ¢æ‰‹ç‡={turnover_rate}% "
                              f"(æ¥æº: {realtime_quote.source.value if hasattr(realtime_quote, 'source') else 'unknown'})")
                else:
                    logger.info(f"{stock_name}({code}) å®æ—¶è¡Œæƒ…è·å–å¤±è´¥æˆ–å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨å†å²æ•°æ®è¿›è¡Œåˆ†æ")
            except Exception as e:
                logger.warning(f"{stock_name}({code}) è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")

            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰åç§°ï¼Œä½¿ç”¨ä»£ç ä½œä¸ºåç§°
            if not stock_name:
                stock_name = f'è‚¡ç¥¨{code}'

            # Step 2: è·å–ç­¹ç åˆ†å¸ƒ - ä½¿ç”¨ç»Ÿä¸€å…¥å£ï¼Œå¸¦ç†”æ–­ä¿æŠ¤
            chip_data = None
            try:
                chip_data = self.fetcher_manager.get_chip_distribution(code)
                if chip_data:
                    logger.info(f"{stock_name}({code}) ç­¹ç åˆ†å¸ƒ: è·åˆ©æ¯”ä¾‹={chip_data.profit_ratio:.1%}, "
                              f"90%é›†ä¸­åº¦={chip_data.concentration_90:.2%}")
                else:
                    logger.debug(f"{stock_name}({code}) ç­¹ç åˆ†å¸ƒè·å–å¤±è´¥æˆ–å·²ç¦ç”¨")
            except Exception as e:
                logger.warning(f"{stock_name}({code}) è·å–ç­¹ç åˆ†å¸ƒå¤±è´¥: {e}")

            # If agent mode is enabled, or specific agent skills are configured, use the Agent analysis pipeline
            use_agent = getattr(self.config, 'agent_mode', False)
            if not use_agent:
                # Auto-enable agent mode when specific skills are configured (e.g., scheduled task with strategy)
                configured_skills = getattr(self.config, 'agent_skills', [])
                if configured_skills and configured_skills != ['all']:
                    use_agent = True
                    logger.info(f"{stock_name}({code}) Auto-enabled agent mode due to configured skills: {configured_skills}")

            if use_agent:
                logger.info(f"{stock_name}({code}) å¯ç”¨ Agent æ¨¡å¼è¿›è¡Œåˆ†æ")
                return self._analyze_with_agent(code, report_type, query_id, stock_name, realtime_quote, chip_data)
            
            # Step 3: è¶‹åŠ¿åˆ†æï¼ˆåŸºäºäº¤æ˜“ç†å¿µï¼‰
            trend_result: Optional[TrendAnalysisResult] = None
            try:
                end_date = date.today()
                start_date = end_date - timedelta(days=89)  # ~60 trading days for MA60
                historical_bars = self.db.get_data_range(code, start_date, end_date)
                if historical_bars:
                    df = pd.DataFrame([bar.to_dict() for bar in historical_bars])
                    # Issue #234: Augment with realtime for intraday MA calculation
                    if self.config.enable_realtime_quote and realtime_quote:
                        df = self._augment_historical_with_realtime(df, realtime_quote, code)
                    trend_result = self.trend_analyzer.analyze(df, code)
                    logger.info(f"{stock_name}({code}) è¶‹åŠ¿åˆ†æ: {trend_result.trend_status.value}, "
                              f"ä¹°å…¥ä¿¡å·={trend_result.buy_signal.value}, è¯„åˆ†={trend_result.signal_score}")
            except Exception as e:
                logger.warning(f"{stock_name}({code}) è¶‹åŠ¿åˆ†æå¤±è´¥: {e}", exc_info=True)

            # Step 4: å¤šç»´åº¦æƒ…æŠ¥æœç´¢ï¼ˆæœ€æ–°æ¶ˆæ¯+é£é™©æ’æŸ¥+ä¸šç»©é¢„æœŸï¼‰
            news_context = None
            if self.search_service.is_available:
                logger.info(f"{stock_name}({code}) å¼€å§‹å¤šç»´åº¦æƒ…æŠ¥æœç´¢...")

                # ä½¿ç”¨å¤šç»´åº¦æœç´¢ï¼ˆæœ€å¤š5æ¬¡æœç´¢ï¼‰
                intel_results = self.search_service.search_comprehensive_intel(
                    stock_code=code,
                    stock_name=stock_name,
                    max_searches=5
                )

                # æ ¼å¼åŒ–æƒ…æŠ¥æŠ¥å‘Š
                if intel_results:
                    news_context = self.search_service.format_intel_report(intel_results, stock_name)
                    total_results = sum(
                        len(r.results) for r in intel_results.values() if r.success
                    )
                    logger.info(f"{stock_name}({code}) æƒ…æŠ¥æœç´¢å®Œæˆ: å…± {total_results} æ¡ç»“æœ")
                    logger.debug(f"{stock_name}({code}) æƒ…æŠ¥æœç´¢ç»“æœ:\n{news_context}")

                    # ä¿å­˜æ–°é—»æƒ…æŠ¥åˆ°æ•°æ®åº“ï¼ˆç”¨äºåç»­å¤ç›˜ä¸æŸ¥è¯¢ï¼‰
                    try:
                        query_context = self._build_query_context(query_id=query_id)
                        for dim_name, response in intel_results.items():
                            if response and response.success and response.results:
                                self.db.save_news_intel(
                                    code=code,
                                    name=stock_name,
                                    dimension=dim_name,
                                    query=response.query,
                                    response=response,
                                    query_context=query_context
                                )
                    except Exception as e:
                        logger.warning(f"{stock_name}({code}) ä¿å­˜æ–°é—»æƒ…æŠ¥å¤±è´¥: {e}")
            else:
                logger.info(f"{stock_name}({code}) æœç´¢æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡æƒ…æŠ¥æœç´¢")

            # Step 5: è·å–åˆ†æä¸Šä¸‹æ–‡ï¼ˆæŠ€æœ¯é¢æ•°æ®ï¼‰
            context = self.db.get_analysis_context(code)

            if context is None:
                logger.warning(f"{stock_name}({code}) æ— æ³•è·å–å†å²è¡Œæƒ…æ•°æ®ï¼Œå°†ä»…åŸºäºæ–°é—»å’Œå®æ—¶è¡Œæƒ…åˆ†æ")
                context = {
                    'code': code,
                    'stock_name': stock_name,
                    'date': date.today().isoformat(),
                    'data_missing': True,
                    'today': {},
                    'yesterday': {}
                }
            
            # Step 6: å¢å¼ºä¸Šä¸‹æ–‡æ•°æ®ï¼ˆæ·»åŠ å®æ—¶è¡Œæƒ…ã€ç­¹ç ã€è¶‹åŠ¿åˆ†æç»“æœã€è‚¡ç¥¨åç§°ï¼‰
            enhanced_context = self._enhance_context(
                context, 
                realtime_quote, 
                chip_data, 
                trend_result,
                stock_name  # ä¼ å…¥è‚¡ç¥¨åç§°
            )
            
            # Step 7: è°ƒç”¨ AI åˆ†æï¼ˆä¼ å…¥å¢å¼ºçš„ä¸Šä¸‹æ–‡å’Œæ–°é—»ï¼‰
            result = self.analyzer.analyze(enhanced_context, news_context=news_context)

            # Step 7.5: å¡«å……åˆ†ææ—¶çš„ä»·æ ¼ä¿¡æ¯åˆ° result
            if result:
                realtime_data = enhanced_context.get('realtime', {})
                result.current_price = realtime_data.get('price')
                result.change_pct = realtime_data.get('change_pct')

            # Step 8: ä¿å­˜åˆ†æå†å²è®°å½•
            if result:
                try:
                    context_snapshot = self._build_context_snapshot(
                        enhanced_context=enhanced_context,
                        news_content=news_context,
                        realtime_quote=realtime_quote,
                        chip_data=chip_data
                    )
                    self.db.save_analysis_history(
                        result=result,
                        query_id=query_id,
                        report_type=report_type.value,
                        news_content=news_context,
                        context_snapshot=context_snapshot,
                        save_snapshot=self.save_context_snapshot
                    )
                except Exception as e:
                    logger.warning(f"{stock_name}({code}) ä¿å­˜åˆ†æå†å²å¤±è´¥: {e}")

            return result

        except Exception as e:
            logger.error(f"{stock_name}({code}) åˆ†æå¤±è´¥: {e}")
            logger.exception(f"{stock_name}({code}) è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            return None
    
    def _enhance_context(
        self,
        context: Dict[str, Any],
        realtime_quote,
        chip_data: Optional[ChipDistribution],
        trend_result: Optional[TrendAnalysisResult],
        stock_name: str = ""
    ) -> Dict[str, Any]:
        """
        å¢å¼ºåˆ†æä¸Šä¸‹æ–‡
        
        å°†å®æ—¶è¡Œæƒ…ã€ç­¹ç åˆ†å¸ƒã€è¶‹åŠ¿åˆ†æç»“æœã€è‚¡ç¥¨åç§°æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
        
        Args:
            context: åŸå§‹ä¸Šä¸‹æ–‡
            realtime_quote: å®æ—¶è¡Œæƒ…æ•°æ®ï¼ˆUnifiedRealtimeQuote æˆ– Noneï¼‰
            chip_data: ç­¹ç åˆ†å¸ƒæ•°æ®
            trend_result: è¶‹åŠ¿åˆ†æç»“æœ
            stock_name: è‚¡ç¥¨åç§°
            
        Returns:
            å¢å¼ºåçš„ä¸Šä¸‹æ–‡
        """
        enhanced = context.copy()
        
        # æ·»åŠ è‚¡ç¥¨åç§°
        if stock_name:
            enhanced['stock_name'] = stock_name
        elif realtime_quote and getattr(realtime_quote, 'name', None):
            enhanced['stock_name'] = realtime_quote.name
        
        # æ·»åŠ å®æ—¶è¡Œæƒ…ï¼ˆå…¼å®¹ä¸åŒæ•°æ®æºçš„å­—æ®µå·®å¼‚ï¼‰
        if realtime_quote:
            # ä½¿ç”¨ getattr å®‰å…¨è·å–å­—æ®µï¼Œç¼ºå¤±å­—æ®µè¿”å› None æˆ–é»˜è®¤å€¼
            volume_ratio = getattr(realtime_quote, 'volume_ratio', None)
            enhanced['realtime'] = {
                'name': getattr(realtime_quote, 'name', ''),
                'price': getattr(realtime_quote, 'price', None),
                'change_pct': getattr(realtime_quote, 'change_pct', None),
                'volume_ratio': volume_ratio,
                'volume_ratio_desc': self._describe_volume_ratio(volume_ratio) if volume_ratio else 'æ— æ•°æ®',
                'turnover_rate': getattr(realtime_quote, 'turnover_rate', None),
                'pe_ratio': getattr(realtime_quote, 'pe_ratio', None),
                'pb_ratio': getattr(realtime_quote, 'pb_ratio', None),
                'total_mv': getattr(realtime_quote, 'total_mv', None),
                'circ_mv': getattr(realtime_quote, 'circ_mv', None),
                'change_60d': getattr(realtime_quote, 'change_60d', None),
                'source': getattr(realtime_quote, 'source', None),
            }
            # ç§»é™¤ None å€¼ä»¥å‡å°‘ä¸Šä¸‹æ–‡å¤§å°
            enhanced['realtime'] = {k: v for k, v in enhanced['realtime'].items() if v is not None}
        
        # æ·»åŠ ç­¹ç åˆ†å¸ƒ
        if chip_data:
            current_price = getattr(realtime_quote, 'price', 0) if realtime_quote else 0
            enhanced['chip'] = {
                'profit_ratio': chip_data.profit_ratio,
                'avg_cost': chip_data.avg_cost,
                'concentration_90': chip_data.concentration_90,
                'concentration_70': chip_data.concentration_70,
                'chip_status': chip_data.get_chip_status(current_price or 0),
            }
        
        # æ·»åŠ è¶‹åŠ¿åˆ†æç»“æœ
        if trend_result:
            enhanced['trend_analysis'] = {
                'trend_status': trend_result.trend_status.value,
                'ma_alignment': trend_result.ma_alignment,
                'trend_strength': trend_result.trend_strength,
                'bias_ma5': trend_result.bias_ma5,
                'bias_ma10': trend_result.bias_ma10,
                'volume_status': trend_result.volume_status.value,
                'volume_trend': trend_result.volume_trend,
                'buy_signal': trend_result.buy_signal.value,
                'signal_score': trend_result.signal_score,
                'signal_reasons': trend_result.signal_reasons,
                'risk_factors': trend_result.risk_factors,
            }

        # Issue #234: Override today with realtime OHLC + trend MA for intraday analysis
        # Guard: trend_result.ma5 > 0 ensures MA calculation succeeded (data sufficient)
        if realtime_quote and trend_result and trend_result.ma5 > 0:
            price = getattr(realtime_quote, 'price', None)
            if price is not None and price > 0:
                yesterday_close = None
                if enhanced.get('yesterday') and isinstance(enhanced['yesterday'], dict):
                    yesterday_close = enhanced['yesterday'].get('close')
                orig_today = enhanced.get('today') or {}
                open_p = getattr(realtime_quote, 'open_price', None) or getattr(
                    realtime_quote, 'pre_close', None
                ) or yesterday_close or orig_today.get('open') or price
                high_p = getattr(realtime_quote, 'high', None) or price
                low_p = getattr(realtime_quote, 'low', None) or price
                vol = getattr(realtime_quote, 'volume', None)
                amt = getattr(realtime_quote, 'amount', None)
                pct = getattr(realtime_quote, 'change_pct', None)
                realtime_today = {
                    'close': price,
                    'open': open_p,
                    'high': high_p,
                    'low': low_p,
                    'ma5': trend_result.ma5,
                    'ma10': trend_result.ma10,
                    'ma20': trend_result.ma20,
                }
                if vol is not None:
                    realtime_today['volume'] = vol
                if amt is not None:
                    realtime_today['amount'] = amt
                if pct is not None:
                    realtime_today['pct_chg'] = pct
                for k, v in orig_today.items():
                    if k not in realtime_today and v is not None:
                        realtime_today[k] = v
                enhanced['today'] = realtime_today
                enhanced['ma_status'] = self._compute_ma_status(
                    price, trend_result.ma5, trend_result.ma10, trend_result.ma20
                )
                enhanced['date'] = date.today().isoformat()
                if yesterday_close is not None:
                    try:
                        yc = float(yesterday_close)
                        if yc > 0:
                            enhanced['price_change_ratio'] = round(
                                (price - yc) / yc * 100, 2
                            )
                    except (TypeError, ValueError):
                        pass
                if vol is not None and enhanced.get('yesterday'):
                    yest_vol = enhanced['yesterday'].get('volume') if isinstance(
                        enhanced['yesterday'], dict
                    ) else None
                    if yest_vol is not None:
                        try:
                            yv = float(yest_vol)
                            if yv > 0:
                                enhanced['volume_change_ratio'] = round(
                                    float(vol) / yv, 2
                                )
                        except (TypeError, ValueError):
                            pass

        # ETF/index flag for analyzer prompt (Fixes #274)
        enhanced['is_index_etf'] = SearchService.is_index_or_etf(
            context.get('code', ''), enhanced.get('stock_name', stock_name)
        )

        return enhanced

    def _analyze_with_agent(
        self, 
        code: str, 
        report_type: ReportType, 
        query_id: str,
        stock_name: str,
        realtime_quote: Any,
        chip_data: Optional[ChipDistribution]
    ) -> Optional[AnalysisResult]:
        """
        ä½¿ç”¨ Agent æ¨¡å¼åˆ†æå•åªè‚¡ç¥¨ã€‚
        """
        try:
            from src.agent.factory import build_agent_executor

            # Build executor from shared factory (ToolRegistry and SkillManager prototype are cached)
            executor = build_agent_executor(self.config, getattr(self.config, 'agent_skills', None) or None)

            # Build initial context to avoid redundant tool calls
            initial_context = {
                "stock_code": code,
                "stock_name": stock_name,
                "report_type": report_type.value,
            }
            
            if realtime_quote:
                initial_context["realtime_quote"] = self._safe_to_dict(realtime_quote)
            if chip_data:
                initial_context["chip_distribution"] = self._safe_to_dict(chip_data)

            # è¿è¡Œ Agent
            message = f"è¯·åˆ†æè‚¡ç¥¨ {code} ({stock_name})ï¼Œå¹¶ç”Ÿæˆå†³ç­–ä»ªè¡¨ç›˜æŠ¥å‘Šã€‚"
            agent_result = executor.run(message, context=initial_context)

            # è½¬æ¢ä¸º AnalysisResult
            result = self._agent_result_to_analysis_result(agent_result, code, stock_name, report_type, query_id)
            resolved_stock_name = result.name if result and result.name else stock_name

            # ä¿å­˜æ–°é—»æƒ…æŠ¥åˆ°æ•°æ®åº“ï¼ˆAgent å·¥å…·ç»“æœä»…ç”¨äº LLM ä¸Šä¸‹æ–‡ï¼ŒæœªæŒä¹…åŒ–ï¼ŒFixes #396ï¼‰
            # ä½¿ç”¨ search_stock_newsï¼ˆä¸ Agent å·¥å…·è°ƒç”¨é€»è¾‘ä¸€è‡´ï¼‰ï¼Œä»… 1 æ¬¡ API è°ƒç”¨ï¼Œæ— é¢å¤–å»¶è¿Ÿ
            if self.search_service.is_available:
                try:
                    news_response = self.search_service.search_stock_news(
                        stock_code=code,
                        stock_name=resolved_stock_name,
                        max_results=5
                    )
                    if news_response.success and news_response.results:
                        query_context = self._build_query_context(query_id=query_id)
                        self.db.save_news_intel(
                            code=code,
                            name=resolved_stock_name,
                            dimension="latest_news",
                            query=news_response.query,
                            response=news_response,
                            query_context=query_context
                        )
                        logger.info(f"[{code}] Agent æ¨¡å¼: æ–°é—»æƒ…æŠ¥å·²ä¿å­˜ {len(news_response.results)} æ¡")
                except Exception as e:
                    logger.warning(f"[{code}] Agent æ¨¡å¼ä¿å­˜æ–°é—»æƒ…æŠ¥å¤±è´¥: {e}")

            # ä¿å­˜åˆ†æå†å²è®°å½•
            if result:
                try:
                    initial_context["stock_name"] = resolved_stock_name
                    self.db.save_analysis_history(
                        result=result,
                        query_id=query_id,
                        report_type=report_type.value,
                        news_content=None,
                        context_snapshot=initial_context,
                        save_snapshot=self.save_context_snapshot
                    )
                except Exception as e:
                    logger.warning(f"[{code}] ä¿å­˜ Agent åˆ†æå†å²å¤±è´¥: {e}")

            return result

        except Exception as e:
            logger.error(f"[{code}] Agent åˆ†æå¤±è´¥: {e}")
            logger.exception(f"[{code}] Agent è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            return None

    def _agent_result_to_analysis_result(
        self, agent_result, code: str, stock_name: str, report_type: ReportType, query_id: str
    ) -> AnalysisResult:
        """
        å°† AgentResult è½¬æ¢ä¸º AnalysisResultã€‚
        """
        result = AnalysisResult(
            code=code,
            name=stock_name,
            sentiment_score=50,
            trend_prediction="æœªçŸ¥",
            operation_advice="è§‚æœ›",
            success=agent_result.success,
            error_message=agent_result.error if not agent_result.success else None,
            data_sources=f"agent:{agent_result.provider}"
        )

        if agent_result.success and agent_result.dashboard:
            dash = agent_result.dashboard
            ai_stock_name = str(dash.get("stock_name", "")).strip()
            if ai_stock_name and self._is_placeholder_stock_name(stock_name, code):
                result.name = ai_stock_name
            result.sentiment_score = self._safe_int(dash.get("sentiment_score"), 50)
            result.trend_prediction = dash.get("trend_prediction", "æœªçŸ¥")
            result.operation_advice = dash.get("operation_advice", "è§‚æœ›")
            result.decision_type = dash.get("decision_type", "hold")
            result.analysis_summary = dash.get("analysis_summary", "")
            # The AI returns a top-level dict that contains a nested 'dashboard' sub-key
            # with core_conclusion / battle_plan / intelligence.  AnalysisResult's helper
            # methods (get_sniper_points, get_core_conclusion, etc.) expect that inner
            # structure, so we unwrap it here.
            result.dashboard = dash.get("dashboard") or dash
        else:
            result.sentiment_score = 50
            result.operation_advice = "è§‚æœ›"
            if not result.error_message:
                result.error_message = "Agent æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å†³ç­–ä»ªè¡¨ç›˜"

        return result

    @staticmethod
    def _is_placeholder_stock_name(name: str, code: str) -> bool:
        """Return True when the stock name is missing or placeholder-like."""
        if not name:
            return True
        normalized = str(name).strip()
        if not normalized:
            return True
        if normalized == code:
            return True
        if normalized.startswith("è‚¡ç¥¨"):
            return True
        if "Unknown" in normalized:
            return True
        return False

    @staticmethod
    def _safe_int(value: Any, default: int = 50) -> int:
        """å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°ã€‚"""
        if value is None:
            return default
        if isinstance(value, int):
            return value
        if isinstance(value, float):
            return int(value)
        if isinstance(value, str):
            import re
            match = re.search(r'-?\d+', value)
            if match:
                return int(match.group())
        return default
    
    def _describe_volume_ratio(self, volume_ratio: float) -> str:
        """
        é‡æ¯”æè¿°
        
        é‡æ¯” = å½“å‰æˆäº¤é‡ / è¿‡å»5æ—¥å¹³å‡æˆäº¤é‡
        """
        if volume_ratio < 0.5:
            return "æåº¦èç¼©"
        elif volume_ratio < 0.8:
            return "æ˜æ˜¾èç¼©"
        elif volume_ratio < 1.2:
            return "æ­£å¸¸"
        elif volume_ratio < 2.0:
            return "æ¸©å’Œæ”¾é‡"
        elif volume_ratio < 3.0:
            return "æ˜æ˜¾æ”¾é‡"
        else:
            return "å·¨é‡"

    @staticmethod
    def _compute_ma_status(close: float, ma5: float, ma10: float, ma20: float) -> str:
        """
        Compute MA alignment status from price and MA values.
        Logic mirrors storage._analyze_ma_status (Issue #234).
        """
        close = close or 0
        ma5 = ma5 or 0
        ma10 = ma10 or 0
        ma20 = ma20 or 0
        if close > ma5 > ma10 > ma20 > 0:
            return "å¤šå¤´æ’åˆ— ğŸ“ˆ"
        elif close < ma5 < ma10 < ma20 and ma20 > 0:
            return "ç©ºå¤´æ’åˆ— ğŸ“‰"
        elif close > ma5 and ma5 > ma10:
            return "çŸ­æœŸå‘å¥½ ğŸ”¼"
        elif close < ma5 and ma5 < ma10:
            return "çŸ­æœŸèµ°å¼± ğŸ”½"
        else:
            return "éœ‡è¡æ•´ç† â†”ï¸"

    def _augment_historical_with_realtime(
        self, df: pd.DataFrame, realtime_quote: Any, code: str
    ) -> pd.DataFrame:
        """
        Augment historical OHLCV with today's realtime quote for intraday MA calculation.
        Issue #234: Use realtime price instead of yesterday's close for technical indicators.
        """
        if df is None or df.empty or 'close' not in df.columns:
            return df
        if realtime_quote is None:
            return df
        price = getattr(realtime_quote, 'price', None)
        if price is None or not (isinstance(price, (int, float)) and price > 0):
            return df

        # Optional: skip augmentation on non-trading days (fail-open)
        enable_realtime_tech = getattr(
            self.config, 'enable_realtime_technical_indicators', True
        )
        if not enable_realtime_tech:
            return df
        market = get_market_for_stock(code)
        if market and not is_market_open(market, date.today()):
            return df

        last_val = df['date'].max()
        last_date = (
            last_val.date() if hasattr(last_val, 'date') else
            (last_val if isinstance(last_val, date) else pd.Timestamp(last_val).date())
        )
        yesterday_close = float(df.iloc[-1]['close']) if len(df) > 0 else price
        open_p = getattr(realtime_quote, 'open_price', None) or getattr(
            realtime_quote, 'pre_close', None
        ) or yesterday_close
        high_p = getattr(realtime_quote, 'high', None) or price
        low_p = getattr(realtime_quote, 'low', None) or price
        vol = getattr(realtime_quote, 'volume', None) or 0
        amt = getattr(realtime_quote, 'amount', None)
        pct = getattr(realtime_quote, 'change_pct', None)

        if last_date >= date.today():
            # Update last row with realtime close (copy to avoid mutating caller's df)
            df = df.copy()
            idx = df.index[-1]
            df.loc[idx, 'close'] = price
            if open_p is not None:
                df.loc[idx, 'open'] = open_p
            if high_p is not None:
                df.loc[idx, 'high'] = high_p
            if low_p is not None:
                df.loc[idx, 'low'] = low_p
            if vol:
                df.loc[idx, 'volume'] = vol
            if amt is not None:
                df.loc[idx, 'amount'] = amt
            if pct is not None:
                df.loc[idx, 'pct_chg'] = pct
        else:
            # Append virtual today row
            new_row = {
                'code': code,
                'date': date.today(),
                'open': open_p,
                'high': high_p,
                'low': low_p,
                'close': price,
                'volume': vol,
                'amount': amt if amt is not None else 0,
                'pct_chg': pct if pct is not None else 0,
            }
            new_df = pd.DataFrame([new_row])
            df = pd.concat([df, new_df], ignore_index=True)
        return df

    def _build_context_snapshot(
        self,
        enhanced_context: Dict[str, Any],
        news_content: Optional[str],
        realtime_quote: Any,
        chip_data: Optional[ChipDistribution]
    ) -> Dict[str, Any]:
        """
        æ„å»ºåˆ†æä¸Šä¸‹æ–‡å¿«ç…§
        """
        return {
            "enhanced_context": enhanced_context,
            "news_content": news_content,
            "realtime_quote_raw": self._safe_to_dict(realtime_quote),
            "chip_distribution_raw": self._safe_to_dict(chip_data),
        }

    @staticmethod
    def _safe_to_dict(value: Any) -> Optional[Dict[str, Any]]:
        """
        å®‰å…¨è½¬æ¢ä¸ºå­—å…¸
        """
        if value is None:
            return None
        if hasattr(value, "to_dict"):
            try:
                return value.to_dict()
            except Exception:
                return None
        if hasattr(value, "__dict__"):
            try:
                return dict(value.__dict__)
            except Exception:
                return None
        return None

    def _resolve_query_source(self, query_source: Optional[str]) -> str:
        """
        è§£æè¯·æ±‚æ¥æºã€‚

        ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
        1. æ˜¾å¼ä¼ å…¥çš„ query_sourceï¼šè°ƒç”¨æ–¹æ˜ç¡®æŒ‡å®šæ—¶ä¼˜å…ˆä½¿ç”¨ï¼Œä¾¿äºè¦†ç›–æ¨æ–­ç»“æœæˆ–å…¼å®¹æœªæ¥ source_message æ¥è‡ªé bot çš„åœºæ™¯
        2. å­˜åœ¨ source_message æ—¶æ¨æ–­ä¸º "bot"ï¼šå½“å‰çº¦å®šä¸ºæœºå™¨äººä¼šè¯ä¸Šä¸‹æ–‡
        3. å­˜åœ¨ query_id æ—¶æ¨æ–­ä¸º "web"ï¼šWeb è§¦å‘çš„è¯·æ±‚ä¼šå¸¦ä¸Š query_id
        4. é»˜è®¤ "system"ï¼šå®šæ—¶ä»»åŠ¡æˆ– CLI ç­‰æ— ä¸Šè¿°ä¸Šä¸‹æ–‡æ—¶

        Args:
            query_source: è°ƒç”¨æ–¹æ˜¾å¼æŒ‡å®šçš„æ¥æºï¼Œå¦‚ "bot" / "web" / "cli" / "system"

        Returns:
            å½’ä¸€åŒ–åçš„æ¥æºæ ‡è¯†å­—ç¬¦ä¸²ï¼Œå¦‚ "bot" / "web" / "cli" / "system"
        """
        if query_source:
            return query_source
        if self.source_message:
            return "bot"
        if self.query_id:
            return "web"
        return "system"

    def _build_query_context(self, query_id: Optional[str] = None) -> Dict[str, str]:
        """
        ç”Ÿæˆç”¨æˆ·æŸ¥è¯¢å…³è”ä¿¡æ¯
        """
        effective_query_id = query_id or self.query_id or ""

        context: Dict[str, str] = {
            "query_id": effective_query_id,
            "query_source": self.query_source or "",
        }

        if self.source_message:
            context.update({
                "requester_platform": self.source_message.platform or "",
                "requester_user_id": self.source_message.user_id or "",
                "requester_user_name": self.source_message.user_name or "",
                "requester_chat_id": self.source_message.chat_id or "",
                "requester_message_id": self.source_message.message_id or "",
                "requester_query": self.source_message.content or "",
            })

        return context
    
    def process_single_stock(
        self,
        code: str,
        skip_analysis: bool = False,
        single_stock_notify: bool = False,
        report_type: ReportType = ReportType.SIMPLE,
        analysis_query_id: Optional[str] = None,
    ) -> Optional[AnalysisResult]:
        """
        å¤„ç†å•åªè‚¡ç¥¨çš„å®Œæ•´æµç¨‹

        åŒ…æ‹¬ï¼š
        1. è·å–æ•°æ®
        2. ä¿å­˜æ•°æ®
        3. AI åˆ†æ
        4. å•è‚¡æ¨é€ï¼ˆå¯é€‰ï¼Œ#55ï¼‰

        æ­¤æ–¹æ³•ä¼šè¢«çº¿ç¨‹æ± è°ƒç”¨ï¼Œéœ€è¦å¤„ç†å¥½å¼‚å¸¸

        Args:
            analysis_query_id: æŸ¥è¯¢é“¾è·¯å…³è” id
            code: è‚¡ç¥¨ä»£ç 
            skip_analysis: æ˜¯å¦è·³è¿‡ AI åˆ†æ
            single_stock_notify: æ˜¯å¦å¯ç”¨å•è‚¡æ¨é€æ¨¡å¼ï¼ˆæ¯åˆ†æå®Œä¸€åªç«‹å³æ¨é€ï¼‰
            report_type: æŠ¥å‘Šç±»å‹æšä¸¾ï¼ˆä»é…ç½®è¯»å–ï¼ŒIssue #119ï¼‰

        Returns:
            AnalysisResult æˆ– None
        """
        logger.info(f"========== å¼€å§‹å¤„ç† {code} ==========")
        
        try:
            # Step 1: è·å–å¹¶ä¿å­˜æ•°æ®
            success, error = self.fetch_and_save_stock_data(code)
            
            if not success:
                logger.warning(f"[{code}] æ•°æ®è·å–å¤±è´¥: {error}")
                # å³ä½¿è·å–å¤±è´¥ï¼Œä¹Ÿå°è¯•ç”¨å·²æœ‰æ•°æ®åˆ†æ
            
            # Step 2: AI åˆ†æ
            if skip_analysis:
                logger.info(f"[{code}] è·³è¿‡ AI åˆ†æï¼ˆdry-run æ¨¡å¼ï¼‰")
                return None
            
            effective_query_id = analysis_query_id or self.query_id or uuid.uuid4().hex
            result = self.analyze_stock(code, report_type, query_id=effective_query_id)
            
            if result:
                logger.info(
                    f"[{code}] åˆ†æå®Œæˆ: {result.operation_advice}, "
                    f"è¯„åˆ† {result.sentiment_score}"
                )
                
                # å•è‚¡æ¨é€æ¨¡å¼ï¼ˆ#55ï¼‰ï¼šæ¯åˆ†æå®Œä¸€åªè‚¡ç¥¨ç«‹å³æ¨é€
                if single_stock_notify and self.notifier.is_available():
                    try:
                        # æ ¹æ®æŠ¥å‘Šç±»å‹é€‰æ‹©ç”Ÿæˆæ–¹æ³•
                        if report_type == ReportType.FULL:
                            # å®Œæ•´æŠ¥å‘Šï¼šä½¿ç”¨å†³ç­–ä»ªè¡¨ç›˜æ ¼å¼
                            report_content = self.notifier.generate_dashboard_report([result])
                            logger.info(f"[{code}] ä½¿ç”¨å®Œæ•´æŠ¥å‘Šæ ¼å¼")
                        else:
                            # ç²¾ç®€æŠ¥å‘Šï¼šä½¿ç”¨å•è‚¡æŠ¥å‘Šæ ¼å¼ï¼ˆé»˜è®¤ï¼‰
                            report_content = self.notifier.generate_single_stock_report(result)
                            logger.info(f"[{code}] ä½¿ç”¨ç²¾ç®€æŠ¥å‘Šæ ¼å¼")
                        
                        if self.notifier.send(report_content, email_stock_codes=[code]):
                            logger.info(f"[{code}] å•è‚¡æ¨é€æˆåŠŸ")
                        else:
                            logger.warning(f"[{code}] å•è‚¡æ¨é€å¤±è´¥")
                    except Exception as e:
                        logger.error(f"[{code}] å•è‚¡æ¨é€å¼‚å¸¸: {e}")
            
            return result
            
        except Exception as e:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œç¡®ä¿å•è‚¡å¤±è´¥ä¸å½±å“æ•´ä½“
            logger.exception(f"[{code}] å¤„ç†è¿‡ç¨‹å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}")
            return None
    
    def run(
        self,
        stock_codes: Optional[List[str]] = None,
        dry_run: bool = False,
        send_notification: bool = True,
        merge_notification: bool = False
    ) -> List[AnalysisResult]:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹

        æµç¨‹ï¼š
        1. è·å–å¾…åˆ†æçš„è‚¡ç¥¨åˆ—è¡¨
        2. ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        3. æ”¶é›†åˆ†æç»“æœ
        4. å‘é€é€šçŸ¥

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è‡ªé€‰è‚¡ï¼‰
            dry_run: æ˜¯å¦ä»…è·å–æ•°æ®ä¸åˆ†æ
            send_notification: æ˜¯å¦å‘é€æ¨é€é€šçŸ¥
            merge_notification: æ˜¯å¦åˆå¹¶æ¨é€ï¼ˆè·³è¿‡æœ¬æ¬¡æ¨é€ï¼Œç”± main å±‚åˆå¹¶ä¸ªè‚¡+å¤§ç›˜åç»Ÿä¸€å‘é€ï¼ŒIssue #190ï¼‰

        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        start_time = time.time()
        
        # ä½¿ç”¨é…ç½®ä¸­çš„è‚¡ç¥¨åˆ—è¡¨
        if stock_codes is None:
            self.config.refresh_stock_list()
            stock_codes = self.config.stock_list
        
        if not stock_codes:
            logger.error("æœªé…ç½®è‡ªé€‰è‚¡åˆ—è¡¨ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® STOCK_LIST")
            return []
        
        logger.info(f"===== å¼€å§‹åˆ†æ {len(stock_codes)} åªè‚¡ç¥¨ =====")
        logger.info(f"è‚¡ç¥¨åˆ—è¡¨: {', '.join(stock_codes)}")
        logger.info(f"å¹¶å‘æ•°: {self.max_workers}, æ¨¡å¼: {'ä»…è·å–æ•°æ®' if dry_run else 'å®Œæ•´åˆ†æ'}")
        
        # === æ‰¹é‡é¢„å–å®æ—¶è¡Œæƒ…ï¼ˆä¼˜åŒ–ï¼šé¿å…æ¯åªè‚¡ç¥¨éƒ½è§¦å‘å…¨é‡æ‹‰å–ï¼‰===
        # åªæœ‰è‚¡ç¥¨æ•°é‡ >= 5 æ—¶æ‰è¿›è¡Œé¢„å–ï¼Œå°‘é‡è‚¡ç¥¨ç›´æ¥é€ä¸ªæŸ¥è¯¢æ›´é«˜æ•ˆ
        if len(stock_codes) >= 5:
            prefetch_count = self.fetcher_manager.prefetch_realtime_quotes(stock_codes)
            if prefetch_count > 0:
                logger.info(f"å·²å¯ç”¨æ‰¹é‡é¢„å–æ¶æ„ï¼šä¸€æ¬¡æ‹‰å–å…¨å¸‚åœºæ•°æ®ï¼Œ{len(stock_codes)} åªè‚¡ç¥¨å…±äº«ç¼“å­˜")
        
        # å•è‚¡æ¨é€æ¨¡å¼ï¼ˆ#55ï¼‰ï¼šä»é…ç½®è¯»å–
        single_stock_notify = getattr(self.config, 'single_stock_notify', False)
        # Issue #119: ä»é…ç½®è¯»å–æŠ¥å‘Šç±»å‹
        report_type_str = getattr(self.config, 'report_type', 'simple').lower()
        report_type = ReportType.FULL if report_type_str == 'full' else ReportType.SIMPLE
        # Issue #128: ä»é…ç½®è¯»å–åˆ†æé—´éš”
        analysis_delay = getattr(self.config, 'analysis_delay', 0)

        if single_stock_notify:
            logger.info(f"å·²å¯ç”¨å•è‚¡æ¨é€æ¨¡å¼ï¼šæ¯åˆ†æå®Œä¸€åªè‚¡ç¥¨ç«‹å³æ¨é€ï¼ˆæŠ¥å‘Šç±»å‹: {report_type_str}ï¼‰")
        
        results: List[AnalysisResult] = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        # æ³¨æ„ï¼šmax_workers è®¾ç½®è¾ƒä½ï¼ˆé»˜è®¤3ï¼‰ä»¥é¿å…è§¦å‘åçˆ¬
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_code = {
                executor.submit(
                    self.process_single_stock,
                    code,
                    skip_analysis=dry_run,
                    single_stock_notify=single_stock_notify and send_notification,
                    report_type=report_type,  # Issue #119: ä¼ é€’æŠ¥å‘Šç±»å‹
                    analysis_query_id=uuid.uuid4().hex,
                ): code
                for code in stock_codes
            }
            
            # æ”¶é›†ç»“æœ
            for idx, future in enumerate(as_completed(future_to_code)):
                code = future_to_code[future]
                try:
                    result = future.result()
                    if result:
                        results.append(result)

                    # Issue #128: åˆ†æé—´éš” - åœ¨ä¸ªè‚¡åˆ†æå’Œå¤§ç›˜åˆ†æä¹‹é—´æ·»åŠ å»¶è¿Ÿ
                    if idx < len(stock_codes) - 1 and analysis_delay > 0:
                        # æ³¨æ„ï¼šæ­¤ sleep å‘ç”Ÿåœ¨â€œä¸»çº¿ç¨‹æ”¶é›† future çš„å¾ªç¯â€ä¸­ï¼Œ
                        # å¹¶ä¸ä¼šé˜»æ­¢çº¿ç¨‹æ± ä¸­çš„ä»»åŠ¡åŒæ—¶å‘èµ·ç½‘ç»œè¯·æ±‚ã€‚
                        # å› æ­¤å®ƒå¯¹é™ä½å¹¶å‘è¯·æ±‚å³°å€¼çš„æ•ˆæœæœ‰é™ï¼›çœŸæ­£çš„å³°å€¼ä¸»è¦ç”± max_workers å†³å®šã€‚
                        # è¯¥è¡Œä¸ºç›®å‰ä¿ç•™ï¼ˆæŒ‰éœ€æ±‚ä¸æ”¹é€»è¾‘ï¼‰ã€‚
                        logger.debug(f"ç­‰å¾… {analysis_delay} ç§’åç»§ç»­ä¸‹ä¸€åªè‚¡ç¥¨...")
                        time.sleep(analysis_delay)

                except Exception as e:
                    logger.error(f"[{code}] ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        
        # ç»Ÿè®¡
        elapsed_time = time.time() - start_time
        
        # dry-run æ¨¡å¼ä¸‹ï¼Œæ•°æ®è·å–æˆåŠŸå³è§†ä¸ºæˆåŠŸ
        if dry_run:
            # æ£€æŸ¥å“ªäº›è‚¡ç¥¨çš„æ•°æ®ä»Šå¤©å·²å­˜åœ¨
            success_count = sum(1 for code in stock_codes if self.db.has_today_data(code))
            fail_count = len(stock_codes) - success_count
        else:
            success_count = len(results)
            fail_count = len(stock_codes) - success_count
        
        logger.info("===== åˆ†æå®Œæˆ =====")
        logger.info(f"æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}, è€—æ—¶: {elapsed_time:.2f} ç§’")
        
        # å‘é€é€šçŸ¥ï¼ˆå•è‚¡æ¨é€æ¨¡å¼ä¸‹è·³è¿‡æ±‡æ€»æ¨é€ï¼Œé¿å…é‡å¤ï¼‰
        if results and send_notification and not dry_run:
            if single_stock_notify:
                # å•è‚¡æ¨é€æ¨¡å¼ï¼šåªä¿å­˜æ±‡æ€»æŠ¥å‘Šï¼Œä¸å†é‡å¤æ¨é€
                logger.info("å•è‚¡æ¨é€æ¨¡å¼ï¼šè·³è¿‡æ±‡æ€»æ¨é€ï¼Œä»…ä¿å­˜æŠ¥å‘Šåˆ°æœ¬åœ°")
                self._send_notifications(results, skip_push=True)
            elif merge_notification:
                # åˆå¹¶æ¨¡å¼ï¼ˆIssue #190ï¼‰ï¼šä»…ä¿å­˜ï¼Œä¸æ¨é€ï¼Œç”± main å±‚åˆå¹¶ä¸ªè‚¡+å¤§ç›˜åç»Ÿä¸€å‘é€
                logger.info("åˆå¹¶æ¨é€æ¨¡å¼ï¼šè·³è¿‡æœ¬æ¬¡æ¨é€ï¼Œå°†åœ¨ä¸ªè‚¡+å¤§ç›˜å¤ç›˜åç»Ÿä¸€å‘é€")
                self._send_notifications(results, skip_push=True)
            else:
                self._send_notifications(results)
        
        return results
    
    def _send_notifications(self, results: List[AnalysisResult], skip_push: bool = False) -> None:
        """
        å‘é€åˆ†æç»“æœé€šçŸ¥
        
        ç”Ÿæˆå†³ç­–ä»ªè¡¨ç›˜æ ¼å¼çš„æŠ¥å‘Š
        
        Args:
            results: åˆ†æç»“æœåˆ—è¡¨
            skip_push: æ˜¯å¦è·³è¿‡æ¨é€ï¼ˆä»…ä¿å­˜åˆ°æœ¬åœ°ï¼Œç”¨äºå•è‚¡æ¨é€æ¨¡å¼ï¼‰
        """
        try:
            logger.info("ç”Ÿæˆå†³ç­–ä»ªè¡¨ç›˜æ—¥æŠ¥...")
            
            # ç”Ÿæˆå†³ç­–ä»ªè¡¨ç›˜æ ¼å¼çš„è¯¦ç»†æ—¥æŠ¥
            report = self.notifier.generate_dashboard_report(results)
            
            # ä¿å­˜åˆ°æœ¬åœ°
            filepath = self.notifier.save_report_to_file(report)
            logger.info(f"å†³ç­–ä»ªè¡¨ç›˜æ—¥æŠ¥å·²ä¿å­˜: {filepath}")
            
            # è·³è¿‡æ¨é€ï¼ˆå•è‚¡æ¨é€æ¨¡å¼ï¼‰
            if skip_push:
                return
            
            # æ¨é€é€šçŸ¥
            if self.notifier.is_available():
                channels = self.notifier.get_available_channels()
                context_success = self.notifier.send_to_context(report)

                # ä¼ä¸šå¾®ä¿¡ï¼šåªå‘ç²¾ç®€ç‰ˆï¼ˆå¹³å°é™åˆ¶ï¼‰
                wechat_success = False
                if NotificationChannel.WECHAT in channels:
                    dashboard_content = self.notifier.generate_wechat_dashboard(results)
                    logger.info(f"ä¼ä¸šå¾®ä¿¡ä»ªè¡¨ç›˜é•¿åº¦: {len(dashboard_content)} å­—ç¬¦")
                    logger.debug(f"ä¼ä¸šå¾®ä¿¡æ¨é€å†…å®¹:\n{dashboard_content}")
                    wechat_success = self.notifier.send_to_wechat(dashboard_content)

                # å…¶ä»–æ¸ é“ï¼šå‘å®Œæ•´æŠ¥å‘Šï¼ˆé¿å…è‡ªå®šä¹‰ Webhook è¢« wechat æˆªæ–­é€»è¾‘æ±¡æŸ“ï¼‰
                non_wechat_success = False
                stock_email_groups = getattr(self.config, 'stock_email_groups', []) or []
                for channel in channels:
                    if channel == NotificationChannel.WECHAT:
                        continue
                    if channel == NotificationChannel.FEISHU:
                        non_wechat_success = self.notifier.send_to_feishu(report) or non_wechat_success
                    elif channel == NotificationChannel.TELEGRAM:
                        non_wechat_success = self.notifier.send_to_telegram(report) or non_wechat_success
                    elif channel == NotificationChannel.EMAIL:
                        if stock_email_groups:
                            code_to_emails: Dict[str, Optional[List[str]]] = {}
                            for r in results:
                                if r.code not in code_to_emails:
                                    emails = []
                                    for stocks, emails_list in stock_email_groups:
                                        if r.code in stocks:
                                            emails.extend(emails_list)
                                    code_to_emails[r.code] = list(dict.fromkeys(emails)) if emails else None
                            emails_to_results: Dict[Optional[Tuple], List] = defaultdict(list)
                            for r in results:
                                recs = code_to_emails.get(r.code)
                                key = tuple(recs) if recs else None
                                emails_to_results[key].append(r)
                            for key, group_results in emails_to_results.items():
                                grp_report = self.notifier.generate_dashboard_report(group_results)
                                if key is None:
                                    non_wechat_success = self.notifier.send_to_email(grp_report) or non_wechat_success
                                else:
                                    non_wechat_success = (
                                        self.notifier.send_to_email(grp_report, receivers=list(key))
                                        or non_wechat_success
                                    )
                        else:
                            non_wechat_success = self.notifier.send_to_email(report) or non_wechat_success
                    elif channel == NotificationChannel.CUSTOM:
                        non_wechat_success = self.notifier.send_to_custom(report) or non_wechat_success
                    elif channel == NotificationChannel.PUSHPLUS:
                        non_wechat_success = self.notifier.send_to_pushplus(report) or non_wechat_success
                    elif channel == NotificationChannel.SERVERCHAN3:
                        non_wechat_success = self.notifier.send_to_serverchan3(report) or non_wechat_success
                    elif channel == NotificationChannel.DISCORD:
                        non_wechat_success = self.notifier.send_to_discord(report) or non_wechat_success
                    elif channel == NotificationChannel.PUSHOVER:
                        non_wechat_success = self.notifier.send_to_pushover(report) or non_wechat_success
                    elif channel == NotificationChannel.ASTRBOT:
                        non_wechat_success = self.notifier.send_to_astrbot(report) or non_wechat_success
                    else:
                        logger.warning(f"æœªçŸ¥é€šçŸ¥æ¸ é“: {channel}")

                success = wechat_success or non_wechat_success or context_success
                if success:
                    logger.info("å†³ç­–ä»ªè¡¨ç›˜æ¨é€æˆåŠŸ")
                else:
                    logger.warning("å†³ç­–ä»ªè¡¨ç›˜æ¨é€å¤±è´¥")
            else:
                logger.info("é€šçŸ¥æ¸ é“æœªé…ç½®ï¼Œè·³è¿‡æ¨é€")
                
        except Exception as e:
            logger.error(f"å‘é€é€šçŸ¥å¤±è´¥: {e}")
